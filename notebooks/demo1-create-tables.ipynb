{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395f44cf",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "This notebook is the first in a series of notebooks created for the Nimbus demo. Here, we will show how to read raw data files from an s3 storage, and ingest them as a table into Trino. Such a table can then be used for further analysis or for creating visualizations in Apache Superset.\n",
    "\n",
    "The notebook also shows how to join two tables on Trino to create a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4bb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import trino\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fe317",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Injecting Credentials\n",
    "\n",
    "In order to run this notebook, we need credentials to connect with S3 storage to retrieve data and the Trino server to create tables.\n",
    "\n",
    "In an automated environment, the credentials can be specified in a pipeline's environment variables or through Openshift secrets.\n",
    "\n",
    "For running the notebook in automation in an elyra pipeline, the environment variables can be updated in the notebook \"Properties\" in the pipeline UI or under `\"env_vars\"` in the `demo1.pipeline yaml` file.\n",
    "\n",
    "For running the notebook in a local environment, we will define them as environment variables in a `credentials.env` file, and load them using dotenv. An example of what the contents of `credentials.env` could look like is shown below\n",
    "\n",
    "```\n",
    "# s3 credentials\n",
    "S3_ENDPOINT=https://s3.us-east-1.amazonaws.com\n",
    "S3_BUCKET=ocp-odh-os-demo-s3\n",
    "S3_ACCESS_KEY=xxx\n",
    "S3_SECRET_KEY=xxx\n",
    "\n",
    "# trino credentials\n",
    "TRINO_USER=xxx\n",
    "TRINO_PASSWD=xxx\n",
    "TRINO_HOST=trino-secure-odh-trino.apps.odh-cl1.apps.os-climate.org\n",
    "TRINO_PORT=443\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84efa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78368e2",
   "metadata": {},
   "source": [
    "## Read Raw Data from S3\n",
    "\n",
    "First, we will read some sample data from s3. We will format the column data types to ensure they can be understood by Trino, as well as rename the columns so that they are compatible with SQL naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466e8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESS_KEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRET_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33197d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample dataset file from s3\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ[\"S3_BUCKET\"],\n",
    "    Key=\"urgentem/UrgentemDataSampleEmissionsTargetsDec2020.csv\",\n",
    ")\n",
    "\n",
    "# load the raw file as a dataframe\n",
    "df_emissions = (pd.read_csv(obj[\"Body\"])).convert_dtypes()\n",
    "\n",
    "# convert columns to specific data types\n",
    "df_emissions = df_emissions.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f085d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download another sample dataset file from s3\n",
    "obj_2 = s3.get_object(\n",
    "    Bucket=os.environ[\"S3_BUCKET\"], Key=\"urgentem/UrgentemDataSampleDec2020.csv\"\n",
    ")\n",
    "\n",
    "# load the raw file as a dataframe\n",
    "df_emissions_2 = (pd.read_csv(obj_2[\"Body\"])).convert_dtypes()\n",
    "\n",
    "# convert columns to specific data types\n",
    "df_emissions_2 = df_emissions_2.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87ac21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methods to clean column names\n",
    "# Author: Erik Erlandson <eje@redhat.com>\n",
    "\n",
    "_wsdedup = re.compile(r\"\\s+\")\n",
    "_usdedup = re.compile(r\"__+\")\n",
    "_rmpunc = re.compile(r\"[,.()&$/+-]+\")\n",
    "_p2smap = {\"string\": \"varchar\", \"Float64\": \"double\", \"Int64\": \"bigint\"}\n",
    "\n",
    "\n",
    "def snakify(name, maxlen):\n",
    "    w = name.casefold().rstrip().lstrip()\n",
    "    w = w.replace(\"-\", \"_\")\n",
    "    w = _rmpunc.sub(\"\", w)\n",
    "    w = _wsdedup.sub(\"_\", w)\n",
    "    w = _usdedup.sub(\"_\", w)\n",
    "    w = w.replace(\"average\", \"avg\")\n",
    "    w = w.replace(\"maximum\", \"max\")\n",
    "    w = w.replace(\"minimum\", \"min\")\n",
    "    w = w.replace(\"absolute\", \"abs\")\n",
    "    w = w.replace(\"source\", \"src\")\n",
    "    w = w.replace(\"distribution\", \"dist\")\n",
    "    # these are common in the sample names but unsure of standard abbv\n",
    "    # w = w.replace(\"inference\", \"inf\")\n",
    "    # w = w.replace(\"emissions\", \"emis\")\n",
    "    # w = w.replace(\"intensity\", \"int\")\n",
    "    # w = w.replace(\"reported\", \"rep\")\n",
    "    # w = w.replace(\"revenue\", \"rev\")\n",
    "    w = w[:maxlen]\n",
    "    return w\n",
    "\n",
    "\n",
    "def snakify_columns(df, inplace=False, maxlen=63):\n",
    "    icols = df.columns.to_list()\n",
    "    ocols = [snakify(e, maxlen=maxlen) for e in icols]\n",
    "    if len(set(ocols)) < len(ocols):\n",
    "        raise ValueError(\"remapped column names were not unique!\")\n",
    "    rename_map = dict(list(zip(icols, ocols)))\n",
    "    return df.rename(columns=rename_map, inplace=inplace)\n",
    "\n",
    "\n",
    "def pandas_type_to_sql(pt):\n",
    "    st = _p2smap.get(pt)\n",
    "    if st is not None:\n",
    "        return st\n",
    "    raise ValueError(\"unexpected pandas column type '{pt}'\".format(pt=pt))\n",
    "\n",
    "\n",
    "# add ability to specify optional dict for specific fields?\n",
    "# if column name is present, use specified value?\n",
    "def generate_table_schema_pairs(df):\n",
    "    ptypes = [str(e) for e in df.dtypes.to_list()]\n",
    "    stypes = [pandas_type_to_sql(e) for e in ptypes]\n",
    "    pz = list(zip(df.columns.to_list(), stypes))\n",
    "    return \",\\n\".join([\"    {n} {t}\".format(n=e[0], t=e[1]) for e in pz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c809b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map column names to a form that works for SQL\n",
    "snakify_columns(df_emissions, inplace=True)\n",
    "\n",
    "# map column names to a form that works for SQL\n",
    "# Had to increase the snakify max length to 100 to avoid column name repetition\n",
    "snakify_columns(df_emissions_2, inplace=True, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5230c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   company_name                        19 non-null     string \n",
      " 1   isin                                19 non-null     string \n",
      " 2   target_type                         19 non-null     string \n",
      " 3   scope                               19 non-null     string \n",
      " 4   coverage_s1                         16 non-null     Float64\n",
      " 5   coverage_s2                         15 non-null     Float64\n",
      " 6   coverage_s3                         4 non-null      Int64  \n",
      " 7   reduction_ambition                  19 non-null     Float64\n",
      " 8   base_year                           19 non-null     Int64  \n",
      " 9   end_year                            19 non-null     Int64  \n",
      " 10  start_year                          19 non-null     Int64  \n",
      " 11  base_year_ghg_emissions_s1_tco2e    1 non-null      string \n",
      " 12  base_year_ghg_emissions_s1s2_tco2e  14 non-null     string \n",
      " 13  base_year_ghg_emissions_s3_tco2e    18 non-null     string \n",
      " 14  achieved_reduction                  19 non-null     Float64\n",
      "dtypes: Float64(4), Int64(4), string(7)\n",
      "memory usage: 2.5 KB\n"
     ]
    }
   ],
   "source": [
    "# a way to examine the structure of a pandas data frame\n",
    "df_emissions.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078824e",
   "metadata": {},
   "source": [
    "## Save Processed Data to S3\n",
    "\n",
    "Now that our data is in a form ingestible by Trino, we will upload it back into our s3 bucket. This will be the data source for our Trino table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b3795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet has multiple options for appending or updating data\n",
    "# including adding new files, or appending, sharding directory trees, etc\n",
    "df_emissions.to_parquet(\"/tmp/emissions_table1.parquet\", index=False)\n",
    "s3.upload_file(\n",
    "    Bucket=os.environ[\"S3_BUCKET\"],\n",
    "    Key=\"urgentem/trino/itr_emissions_join_1/emissions.parquet\",\n",
    "    Filename=\"/tmp/emissions_table1.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd04263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet has multiple options for appending or updating data\n",
    "# including adding new files, or appending, sharding directory trees, etc\n",
    "df_emissions_2.to_parquet(\"/tmp/emissions_table2.parquet\", index=False)\n",
    "s3.upload_file(\n",
    "    Bucket=os.environ[\"S3_BUCKET\"],\n",
    "    Key=\"urgentem/trino/itr_emissions_join_2/emissions.parquet\",\n",
    "    Filename=\"/tmp/emissions_table2.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8af31",
   "metadata": {},
   "source": [
    "## Create a Table on Trino\n",
    "\n",
    "Finally, we will create a table in our Trino database that uses the parquet files we uploaded in the previous section as the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950868a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trino password env-var to hold token values\n",
    "JWT_TOKEN = os.environ['TRINO_PASSWD']\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=os.environ['TRINO_PORT'],\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(JWT_TOKEN),\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4860b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a sql schema that will correspond to the data types\n",
    "# of columns in the pandas DF\n",
    "# to-do: add some mechanisms for overriding types, either here\n",
    "# or on the pandas data-frame itself before we write it out\n",
    "schema = generate_table_schema_pairs(df_emissions)\n",
    "\n",
    "tabledef = \"\"\"create table if not exists osc_datacommons_dev.urgentem.itr_emissions_1(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{s3_bucket}/urgentem/trino/itr_emissions_join_1/'\n",
    ")\"\"\".format(\n",
    "    schema=schema,\n",
    "    s3_bucket=os.environ[\"S3_BUCKET\"],\n",
    ")\n",
    "\n",
    "# tables created externally may not show up immediately in cloud-beaver\n",
    "cur.execute(tabledef)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b235749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1  # generate a sql schema that will correspond to the data types\n",
    "# of columns in the pandas DF\n",
    "# to-do: add some mechanisms for overriding types, either here\n",
    "# or on the pandas data-frame itself before we write it out\n",
    "schema = generate_table_schema_pairs(df_emissions_2)\n",
    "\n",
    "tabledef = \"\"\"create table if not exists osc_datacommons_dev.urgentem.itr_emissions_2(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{s3_bucket}/urgentem/trino/itr_emissions_join_2/'\n",
    ")\"\"\".format(\n",
    "    schema=schema,\n",
    "    s3_bucket=os.environ[\"S3_BUCKET\"],\n",
    ")\n",
    "\n",
    "# tables created externally may not show up immediately in cloud-beaver\n",
    "cur.execute(tabledef)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee568250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DANONE',\n",
       " 'FR0000120644',\n",
       " 'Absolute',\n",
       " 'S1+S2',\n",
       " 0.95,\n",
       " 0.95,\n",
       " None,\n",
       " 0.3,\n",
       " 2015,\n",
       " 2030,\n",
       " 2017,\n",
       " None,\n",
       " ' 1,681,235 ',\n",
       " ' 8,406,175 ',\n",
       " 0.68]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if table 1 is there\n",
    "cur.execute(\"select * from osc_datacommons_dev.urgentem.itr_emissions_1 LIMIT 5\")\n",
    "cur.fetchall()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f526a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BANCO SANTANDER',\n",
       " 'ES0113900J37',\n",
       " '5705946',\n",
       " 'SAN SM',\n",
       " 1,\n",
       " 2,\n",
       " 'Spain',\n",
       " 'Europe',\n",
       " 693.1,\n",
       " 4.1,\n",
       " 'Sum of Market and Scope One',\n",
       " 689.1,\n",
       " 'Sum of Average Category Intensities',\n",
       " 0.5,\n",
       " 'Reported']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if table 2 is there\n",
    "cur.execute(\"select * from osc_datacommons_dev.urgentem.itr_emissions_2 LIMIT 5\")\n",
    "cur.fetchall()[1][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d0a0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we showed how to take raw csv files on S3, process them into a format usable by Trino, and save them as parquet files on S3. Then we showed how to create tables on Trino using these parquet files. The tables can now be used in a Superset dashboard for visualization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
