{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "25054789-3a79-4d61-b2c3-31b9f07335eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b9dd11e-8709-4e93-97f9-c7c80f528352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import config\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from farm.infer import Inferencer\n",
    "from config_farm_train import InferConfig\n",
    "from src.data.s3_communication import S3Communication, S3FileType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47192225-f899-4565-b807-b7949174cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.relevance_infer import TextRelevanceInfer\n",
    "from src.components.utils.kpi_mapping import get_kpi_mapping_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8f47b11-76dd-47aa-b5b1-8b28157fb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from farm.infer import QAInferencer\n",
    "from config_qa_farm_train import QAFileConfig, QAInferConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85257c-4eae-46ad-8e6d-be36a1c1457e",
   "metadata": {},
   "source": [
    "## 1. Setup for benchmark runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70445754-6b25-449d-95ef-e874934e1fa7",
   "metadata": {},
   "source": [
    "### 1.1. Load S3 credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ec3c193-d14b-4338-827d-20a47153a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d37df1-d489-452e-a7a7-61921fc7c0ed",
   "metadata": {},
   "source": [
    "### 1.2. Creating S3 connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44b4a612-75ac-4159-8cae-9a7154e38a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_LANDING_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"S3_LANDING_ACCESS_KEY\"),\n",
    "    aws_secret_access_key=os.getenv(\"S3_LANDING_SECRET_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_LANDING_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b772337-1caa-4690-a115-e73ea2393523",
   "metadata": {},
   "source": [
    "### 1.3. View Objects in the bucket"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d783a71b-ccb6-4e52-87bf-c3a7f7022202",
   "metadata": {},
   "source": [
    "obj = s3c.s3_resource.Bucket(s3c.bucket).objects.filter(Prefix=f\"\")\n",
    "for i in obj:\n",
    "    if \"kpi_val_split.csv\" in str(i):\n",
    "        print(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f05c6-5f32-4d57-9954-cdcb731d6b3b",
   "metadata": {},
   "source": [
    "### 1.4. Base Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "26fd21b9-b3f5-40aa-b202-d5ee5794e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_config = InferConfig(\"infer_demo\")\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10c8e5-66d7-4015-beb8-3389a7107f32",
   "metadata": {},
   "source": [
    "### 1.5. Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68abca80-b77f-4889-a9cc-6583bef0ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pretrained model\n",
    "model_root = pathlib.Path(infer_config.load_dir['Text']).parent\n",
    "model_rel_zip = pathlib.Path(model_root, \"relevance_roberta.zip\")\n",
    "\n",
    "s3c.download_file_from_s3(\n",
    "    model_rel_zip, config.CHECKPOINT_S3_PREFIX, \"relevance_roberta.zip\"\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(pathlib.Path(model_root, \"relevance_roberta.zip\"), \"r\") as z:\n",
    "    z.extractall(model_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62984a2-811b-4563-9acf-8a70f0ba52e2",
   "metadata": {},
   "source": [
    "### 1.6. Loading KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e325371-4d8c-4471-af2e-706a0b4e6f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kpi_id</th>\n",
       "      <th>question</th>\n",
       "      <th>sectors</th>\n",
       "      <th>add_year</th>\n",
       "      <th>kpi_category</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>What is the company name?</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>In which year was the annual report or the sus...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>What is the total volume of proven and probabl...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1</td>\n",
       "      <td>What is the volume of estimated proven hydroca...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>What is the volume of estimated probable hydro...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>What is the total volume of hydrocarbons produ...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.1</td>\n",
       "      <td>What is the total volume of crude oil liquid p...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.2</td>\n",
       "      <td>What is the total volume of natural gas liquid...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.3</td>\n",
       "      <td>What is the total volume of natural gas produc...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>What is the annual total production from coal?</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.1</td>\n",
       "      <td>What is the annual total production from ligni...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.2</td>\n",
       "      <td>What is the annual total production from hard ...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>What is the total installed capacity from coal?</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.1</td>\n",
       "      <td>What is the total installed capacity from lign...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.2</td>\n",
       "      <td>What is the total installed capacity from hard...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.0</td>\n",
       "      <td>What is the total amount of direct greenhouse ...</td>\n",
       "      <td>CU, OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>What is the total amount of energy indirect gr...</td>\n",
       "      <td>CU, OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>What is the total amount of upstream energy in...</td>\n",
       "      <td>CU, OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.0</td>\n",
       "      <td>What is the base year for carbon reduction com...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>What is the climate commitment scenario consid...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.0</td>\n",
       "      <td>What is the target year for climate commitment?</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.0</td>\n",
       "      <td>What is the target carbon reduction in percent...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.0</td>\n",
       "      <td>What is the total amount of scope 1 and 2 gree...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.0</td>\n",
       "      <td>What is the total amount of scope 1, scope 2 a...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kpi_id                                           question     sectors  \\\n",
       "0      0.0                          What is the company name?  OG, CM, CU   \n",
       "1      1.0  In which year was the annual report or the sus...  OG, CM, CU   \n",
       "2      2.0  What is the total volume of proven and probabl...          OG   \n",
       "3      2.1  What is the volume of estimated proven hydroca...          OG   \n",
       "4      2.2  What is the volume of estimated probable hydro...          OG   \n",
       "5      3.0  What is the total volume of hydrocarbons produ...          OG   \n",
       "6      3.1  What is the total volume of crude oil liquid p...          OG   \n",
       "7      3.2  What is the total volume of natural gas liquid...          OG   \n",
       "8      3.3  What is the total volume of natural gas produc...          OG   \n",
       "9      4.0     What is the annual total production from coal?          CU   \n",
       "10     4.1  What is the annual total production from ligni...          CU   \n",
       "11     4.2  What is the annual total production from hard ...          CU   \n",
       "12     5.0    What is the total installed capacity from coal?          CU   \n",
       "13     5.1  What is the total installed capacity from lign...          CU   \n",
       "14     5.2  What is the total installed capacity from hard...          CU   \n",
       "15     6.0  What is the total amount of direct greenhouse ...      CU, OG   \n",
       "16     7.0  What is the total amount of energy indirect gr...      CU, OG   \n",
       "17     8.0  What is the total amount of upstream energy in...      CU, OG   \n",
       "18     9.0  What is the base year for carbon reduction com...  OG, CM, CU   \n",
       "19    10.0  What is the climate commitment scenario consid...  OG, CM, CU   \n",
       "20    11.0    What is the target year for climate commitment?  OG, CM, CU   \n",
       "21    12.0  What is the target carbon reduction in percent...  OG, CM, CU   \n",
       "22    13.0  What is the total amount of scope 1 and 2 gree...          CU   \n",
       "23    14.0  What is the total amount of scope 1, scope 2 a...          OG   \n",
       "\n",
       "    add_year kpi_category  Unnamed: 5  Unnamed: 6  \n",
       "0      False         TEXT         NaN         NaN  \n",
       "1      False         TEXT         NaN         NaN  \n",
       "2       True  TEXT, TABLE         NaN         NaN  \n",
       "3       True  TEXT, TABLE         NaN         NaN  \n",
       "4       True  TEXT, TABLE         NaN         NaN  \n",
       "5       True  TEXT, TABLE         NaN         NaN  \n",
       "6       True  TEXT, TABLE         NaN         NaN  \n",
       "7       True  TEXT, TABLE         NaN         NaN  \n",
       "8       True  TEXT, TABLE         NaN         NaN  \n",
       "9       True  TEXT, TABLE         NaN         NaN  \n",
       "10      True  TEXT, TABLE         NaN         NaN  \n",
       "11      True  TEXT, TABLE         NaN         NaN  \n",
       "12      True  TEXT, TABLE         NaN         NaN  \n",
       "13      True  TEXT, TABLE         NaN         NaN  \n",
       "14      True  TEXT, TABLE         NaN         NaN  \n",
       "15      True  TEXT, TABLE         NaN         NaN  \n",
       "16      True  TEXT, TABLE         NaN         NaN  \n",
       "17      True  TEXT, TABLE         NaN         NaN  \n",
       "18     False  TEXT, TABLE         NaN         NaN  \n",
       "19      True         TEXT         NaN         NaN  \n",
       "20     False         TEXT         NaN         NaN  \n",
       "21      True         TEXT         NaN         NaN  \n",
       "22      True  TEXT, TABLE         NaN         NaN  \n",
       "23      True  TEXT, TABLE         NaN         NaN  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpi_df = s3c.download_df_from_s3(\n",
    "    \"aicoe-osc-demo/kpi_mapping\",\n",
    "    \"kpi_mapping.csv\",\n",
    "    filetype=S3FileType.CSV,\n",
    "    header=0,\n",
    ")\n",
    "kpi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21ff4b-8620-4a53-bb0e-b918166b16e6",
   "metadata": {},
   "source": [
    "## 2. Running Benchmarks on Relevance Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71beed6-2e42-4a81-a3b9-ffbe1fc300a9",
   "metadata": {},
   "source": [
    "### 2.1. Setting up scratch directories and files for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "204236e3-c544-4fe7-afa0-a37019afb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_FOLDER = config.DATA_FOLDER / \"benchmark\"\n",
    "if not os.path.exists(BENCHMARK_FOLDER):\n",
    "    BENCHMARK_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(BENCHMARK_FOLDER / \"extraction\"):\n",
    "    pathlib.Path(BENCHMARK_FOLDER / \"extraction\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(BENCHMARK_FOLDER / \"infer_relevance\"):\n",
    "    pathlib.Path(BENCHMARK_FOLDER / \"infer_relevance\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "s3c.download_files_in_prefix_to_dir(\n",
    "        config.BASE_EXTRACTION_S3_PREFIX,\n",
    "        BENCHMARK_FOLDER / \"extraction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a24c72-4258-4b57-949f-93085c2c8eab",
   "metadata": {},
   "source": [
    "### 2.2. Relevance Infer Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02c92125-8b40-4dda-8520-ad4db328eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': '/opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/models/RELEVANCE'}\n",
      "/opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/extraction\n",
      "/opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance\n"
     ]
    }
   ],
   "source": [
    "infer_config.extracted_dir = BENCHMARK_FOLDER / \"extraction\"\n",
    "infer_config.result_dir = BENCHMARK_FOLDER / \"infer_relevance\"\n",
    "print(infer_config.load_dir)\n",
    "print(infer_config.extracted_dir)\n",
    "print(infer_config.result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9224841-88af-4da4-8089-055e15a341f9",
   "metadata": {},
   "source": [
    "### 2.3. Defining Methods from the TextRelevanceInfer class for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e91bba99-377f-42e3-b1be-f0cd9cfa6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_relevance_infer():\n",
    "        \"\"\"Initialize BaseRelevanceInfer class.\"\"\"\n",
    "        data_type = \"Text\"\n",
    "        global questions\n",
    "        global model\n",
    "        # Questions can be set in the config file. If not provided, the prediction will be made for all KPI questions\n",
    "        if len(infer_config.kpi_questions) > 0:\n",
    "            questions = infer_config.kpi_questions\n",
    "        else:\n",
    "            # Filter KPIs based on section and whether they can be found in text or table.\n",
    "            kmc = get_kpi_mapping_category(kpi_df)\n",
    "            questions = [\n",
    "                q_text\n",
    "                for q_id, (q_text, sect) in kmc[\"KPI_MAPPING_MODEL\"].items()\n",
    "                if len(set(sect).intersection(set(infer_config.sectors))) > 0\n",
    "                and data_type.upper() in kmc[\"KPI_CATEGORY\"][q_id]\n",
    "            ]\n",
    "\n",
    "        if not os.path.exists(infer_config.result_dir):\n",
    "            os.makedirs(infer_config.result_dir)\n",
    "\n",
    "        farm_logger = logging.getLogger(\"farm\")\n",
    "        farm_logger.setLevel(infer_config.farm_infer_logging_level)\n",
    "        model = Inferencer.load(\n",
    "            infer_config.load_dir[data_type],\n",
    "            batch_size=infer_config.batch_size,\n",
    "            gpu=infer_config.gpu,\n",
    "            num_processes=infer_config.num_processes,\n",
    "            disable_tqdm=infer_config.disable_tqdm,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "46d9b0e1-40f9-4ece-8fc6-06769dd26eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_json(file):\n",
    "        \"\"\"Read text from json.\"\"\"\n",
    "        with open(file) as f:\n",
    "            text = json.load(f)\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd9969d7-cf21-4177-aa4c-fcfdf29ac4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(pdf_name, pdf_path):\n",
    "        \"\"\"Gather all the text data inside the given pdf and prepares it to be passed to text model.\n",
    "\n",
    "        Args:\n",
    "            pdf_name (str): Name of the pdf\n",
    "            pdf_path (str): Path to the pdf\n",
    "        Returns:\n",
    "            text_data (A list of a list of dicts): The dict has \"page\", \"pdf_name\",\n",
    "                                                    \"text\", \"text_b\" keys.\n",
    "        \"\"\"\n",
    "        # Get all the extracted pdf text from the json file in the extracted folder\n",
    "        pdf_content = read_text_from_json(pdf_path)\n",
    "        text_data = []\n",
    "        # build all possible combinations of paragraphs and  questions\n",
    "        # Keep track of page number which the text is extracted from and the pdf it belongs to.\n",
    "        for kpi_question in questions:\n",
    "            text_data.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"page\": page_num,\n",
    "                        \"pdf_name\": pdf_name,\n",
    "                        \"text\": kpi_question,\n",
    "                        \"text_b\": paragraph,\n",
    "                    }\n",
    "                    for page_num, page_content in pdf_content.items()\n",
    "                    for paragraph in page_content\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        _logger.info(\n",
    "            \"###### Received {} examples for Text, number of questions: {}\".format(\n",
    "                int(len(text_data) / len(questions)), len(questions)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80d44e35-83e6-42a8-9722-a1ea8b96ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_extracted_files():\n",
    "        \"\"\"Gather all the extracted texts for each pdf.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary where the keys are the pdf names and the values are the path to the json files containing\n",
    "            the extracted text for each pdf\n",
    "        \"\"\"\n",
    "        # Get all the json extracted from pdfs which are located in extracted folder\n",
    "        _logger.info(\"Searching for extracted files on {}\".format(infer_config.extracted_dir))\n",
    "        text_paths = sorted(Path(infer_config.extracted_dir).rglob(\"*.json\"))\n",
    "        return {\n",
    "            os.path.splitext(os.path.basename(file_path))[0]: file_path\n",
    "            for file_path in text_paths\n",
    "            if \"table_meta\" not in str(file_path)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "46f87db0-2ade-4bbb-93fe-0fdaf70a05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_folder():\n",
    "        \"\"\"Make prediction on all the data (csv files or json) inside a folder.\n",
    "\n",
    "        It also saves the relevant tables or\n",
    "        paragraphs for questions inside a csv file.\n",
    "        \"\"\"\n",
    "        data_type = \"Text\"\n",
    "        all_text_path_dict = gather_extracted_files()\n",
    "        df_list = []\n",
    "        metrics_df_list = []\n",
    "        num_pdfs = len(all_text_path_dict)\n",
    "        _logger.info(\n",
    "            \"{} Starting Relevence Inference for the following extracted pdf files found in {}:\\n{} \".format(\n",
    "                \"#\" * 20, infer_config.result_dir, [pdf for pdf in all_text_path_dict.keys()]\n",
    "            )\n",
    "        )\n",
    "        for i, (pdf_name, file_path) in enumerate(all_text_path_dict.items()):\n",
    "            _logger.info(\"{} {}/{} PDFs\".format(\"#\" * 20, i + 1, num_pdfs))\n",
    "            predictions_file_name = \"{}_{}\".format(pdf_name, \"predictions_relevant.csv\")\n",
    "            if (\n",
    "                infer_config.skip_processed_files\n",
    "                and predictions_file_name in os.listdir(infer_config.result_dir)\n",
    "            ):\n",
    "                _logger.info(\n",
    "                    \"The relevance infer results for {} already exists. Skipping.\".format(\n",
    "                        pdf_name\n",
    "                    )\n",
    "                )\n",
    "                _logger.info(\n",
    "                    \"If you would like to re-process the already processed files, set \"\n",
    "                    \"`skip_processed_files` to False in the config file. \"\n",
    "                )\n",
    "                continue\n",
    "            _logger.info(\"Running inference for {}:\".format(pdf_name))\n",
    "\n",
    "            try:\n",
    "                start = time.time()\n",
    "                data = gather_data(pdf_name, file_path)\n",
    "                num_data_points = len(data)\n",
    "                num_pages = data[len(data)-1]['page']\n",
    "                _logger.info(\n",
    "                    \"Gathered the extracted data ({} points) from the file {} in {} sec.\".format(num_data_points, pdf_name, str(time.time() - start))\n",
    "                )\n",
    "                predictions = []\n",
    "                chunk_size = 1000\n",
    "                chunk_idx = 0\n",
    "                total_file_time = 0\n",
    "        \n",
    "                while chunk_idx * chunk_size < num_data_points:\n",
    "                    chunk_start = time.time()\n",
    "                    data_chunk = data[\n",
    "                        chunk_idx * chunk_size : (chunk_idx + 1) * chunk_size\n",
    "                    ]\n",
    "                    predictions_chunk = model.inference_from_dicts(\n",
    "                        dicts=data_chunk\n",
    "                    )\n",
    "                    \n",
    "                    predictions.extend(predictions_chunk)\n",
    "                    chunk_idx += 1\n",
    "                    \n",
    "                    chunk_end = time.time()\n",
    "                    total_file_time += (chunk_end - chunk_start)\n",
    "\n",
    "                time_per_data_point = total_file_time / num_data_points\n",
    "                data_points_per_sec = 1/time_per_data_point\n",
    "                _logger.info(\n",
    "                    \"Ran inference on file {} with {} pages and {} data points in {} sec ({} sec per data point, {} data points per second)\".format(\n",
    "                        pdf_name, num_pages, num_data_points, total_file_time, time_per_data_point, data_points_per_sec\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                metrics_list = [[pdf_name, int(num_pages), num_data_points, total_file_time, time_per_data_point, data_points_per_sec]]\n",
    "                metrics_df = pd.DataFrame(metrics_list, columns = ['PDF Name', 'Number of Pages', 'Number of Data Points', 'Total Inference Time', 'Time per data point', 'Data points per sec'])\n",
    "                metrics_df_list.append(metrics_df)\n",
    "                \n",
    "                flat_predictions = [\n",
    "                    example for batch in predictions for example in batch[\"predictions\"]\n",
    "                ]\n",
    "                positive_examples = [\n",
    "                    data[index]\n",
    "                    for index, pred_example in enumerate(flat_predictions)\n",
    "                    if pred_example[\"label\"] == \"1\"\n",
    "                ]\n",
    "                \n",
    "                df = pd.DataFrame(positive_examples)\n",
    "                df[\"source\"] = data_type\n",
    "\n",
    "                df_list.append(df)\n",
    "                predictions_file_path = os.path.join(\n",
    "                    infer_config.result_dir, predictions_file_name\n",
    "                )\n",
    "                df.to_csv(predictions_file_path)\n",
    "                _logger.info(\n",
    "                    \"Saved {} relevant {} examples for {} in {}\".format(\n",
    "                        len(df), data_type, pdf_name, predictions_file_path\n",
    "                    )\n",
    "                )\n",
    "            except Exception as exc:\n",
    "                _logger.warning(exc)\n",
    "                e = sys.exc_info()[0]\n",
    "                _logger.warning(\n",
    "                    \"There was an error making inference (RELEVANCE) on {}\".format(\n",
    "                        pdf_name\n",
    "                    )\n",
    "                )\n",
    "                _logger.warning(\"The error is\\n{}\\nSkipping this pdf\".format(e))\n",
    "\n",
    "        concatenated_dfs = pd.concat(df_list) if len(df_list) > 0 else pd.DataFrame()\n",
    "        metrics_df = pd.DataFrame()\n",
    "        if len(metrics_df_list) > 0:\n",
    "            metrics_df = pd.concat(metrics_df_list) if len(metrics_df_list) > 0 else pd.DataFrame()\n",
    "            _logger.info(\n",
    "                \"Metrics for inferring paragraphs relevant to KPI are: \\nTotal Number of Data Points Processed = {} \\nTotal Inference Time = {} \\nAverage Number of Pages Per PDF = {} \\nAverage Inference Time Per PDF = {} \\nMinimum Inference Time of PDF = {} \\nMaximum Inference Time of PDF = {} \\nStd of Inference Times of PDFs = {} \\nAverage Time Per Data Point Processed= {} \\nAverage Data Points Processed Per Second = {} \\n\"\n",
    "                .format(metrics_df['Number of Data Points'].sum(), \n",
    "                        metrics_df['Total Inference Time'].sum(),\n",
    "                        int(metrics_df['Number of Pages'].mean()),\n",
    "                        metrics_df['Total Inference Time'].mean(),\n",
    "                        metrics_df['Total Inference Time'].min(),\n",
    "                        metrics_df['Total Inference Time'].max(),\n",
    "                        metrics_df['Total Inference Time'].std(),\n",
    "                        metrics_df['Time per data point'].mean(),\n",
    "                        metrics_df['Data points per sec'].mean()\n",
    "                        )\n",
    "            )\n",
    "        \n",
    "        model.close_multiprocessing_pool()\n",
    "        return concatenated_dfs, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4841d686-85a8-4d21-a226-562da87f3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_for_relevance():\n",
    "    if not os.path.exists(BENCHMARK_FOLDER / \"infer_relevance\"):\n",
    "        pathlib.Path(BENCHMARK_FOLDER / \"infer_relevance\").mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(BENCHMARK_FOLDER / \"infer_relevance\"):\n",
    "        file_path = os.path.join(BENCHMARK_FOLDER / \"infer_relevance\", filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5153cc-78d5-4065-a6f5-c2c466a6da44",
   "metadata": {},
   "source": [
    "### 2.4. Running benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f20cb29-768e-47c8-bd99-1d656ba6a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/08/2022 09:19:37 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/08/2022 09:19:41 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/08/2022 09:19:42 - INFO - __main__ -   Searching for extracted files on /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/extraction\n",
      "08/08/2022 09:19:42 - INFO - __main__ -   #################### Starting Relevence Inference for the following extracted pdf files found in /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance:\n",
      "['04_NOVATEK_AR_2016_ENG_11'] \n",
      "08/08/2022 09:19:42 - INFO - __main__ -   #################### 1/1 PDFs\n",
      "08/08/2022 09:19:42 - INFO - __main__ -   Running inference for 04_NOVATEK_AR_2016_ENG_11:\n",
      "08/08/2022 09:19:42 - INFO - __main__ -   ###### Received 1011 examples for Text, number of questions: 24\n",
      "08/08/2022 09:19:42 - INFO - __main__ -   Gathered the extracted data (24264 points) from the file 04_NOVATEK_AR_2016_ENG_11 in 0.016501188278198242 sec.\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "08/08/2022 09:22:22 - INFO - __main__ -   Ran inference on file 04_NOVATEK_AR_2016_ENG_11 with 119 pages and 24264 data points in 160.24668073654175 sec (0.006604297755380059 sec per data point, 151.41655283264143 data points per second)\n",
      "08/08/2022 09:22:22 - INFO - __main__ -   Saved 404 relevant Text examples for 04_NOVATEK_AR_2016_ENG_11 in /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance/04_NOVATEK_AR_2016_ENG_11_predictions_relevant.csv\n",
      "08/08/2022 09:22:22 - INFO - __main__ -   Metrics for inferring paragraphs relevant to KPI are: \n",
      "Total Number of Data Points Processed = 24264 \n",
      "Total Inference Time = 160.24668073654175 \n",
      "Average Number of Pages Per PDF = 119 \n",
      "Average Inference Time Per PDF = 160.24668073654175 \n",
      "Minimum Inference Time of PDF = 160.24668073654175 \n",
      "Maximum Inference Time of PDF = 160.24668073654175 \n",
      "Std of Inference Times of PDFs = nan \n",
      "Average Time Per Data Point Processed= 0.006604297755380059 \n",
      "Average Data Points Processed Per Second = 151.41655283264143 \n",
      "\n",
      "08/08/2022 09:22:26 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/08/2022 09:22:26 - INFO - __main__ -   Searching for extracted files on /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/extraction\n",
      "08/08/2022 09:22:26 - INFO - __main__ -   #################### Starting Relevence Inference for the following extracted pdf files found in /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance:\n",
      "['04_NOVATEK_AR_2016_ENG_11'] \n",
      "08/08/2022 09:22:26 - INFO - __main__ -   #################### 1/1 PDFs\n",
      "08/08/2022 09:22:26 - INFO - __main__ -   Running inference for 04_NOVATEK_AR_2016_ENG_11:\n",
      "08/08/2022 09:22:26 - INFO - __main__ -   ###### Received 1011 examples for Text, number of questions: 24\n",
      "08/08/2022 09:22:26 - INFO - __main__ -   Gathered the extracted data (24264 points) from the file 04_NOVATEK_AR_2016_ENG_11 in 0.01685643196105957 sec.\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "08/08/2022 09:25:07 - INFO - __main__ -   Ran inference on file 04_NOVATEK_AR_2016_ENG_11 with 119 pages and 24264 data points in 160.22421050071716 sec (0.006603371682357285 sec per data point, 151.43778786097621 data points per second)\n",
      "08/08/2022 09:25:07 - INFO - __main__ -   Saved 404 relevant Text examples for 04_NOVATEK_AR_2016_ENG_11 in /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance/04_NOVATEK_AR_2016_ENG_11_predictions_relevant.csv\n",
      "08/08/2022 09:25:07 - INFO - __main__ -   Metrics for inferring paragraphs relevant to KPI are: \n",
      "Total Number of Data Points Processed = 24264 \n",
      "Total Inference Time = 160.22421050071716 \n",
      "Average Number of Pages Per PDF = 119 \n",
      "Average Inference Time Per PDF = 160.22421050071716 \n",
      "Minimum Inference Time of PDF = 160.22421050071716 \n",
      "Maximum Inference Time of PDF = 160.22421050071716 \n",
      "Std of Inference Times of PDFs = nan \n",
      "Average Time Per Data Point Processed= 0.006603371682357285 \n",
      "Average Data Points Processed Per Second = 151.43778786097621 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 2\n",
    "metrics_dfs = []\n",
    "initialize_relevance_infer()\n",
    "for i in range(num_runs):\n",
    "    cleanup_for_relevance()\n",
    "    initialize_relevance_infer()\n",
    "    result_df, metrics_df = run_folder()\n",
    "    metrics_dfs.append(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "abdad085-786a-4402-bac7-2765f7e6d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    PDF Name  Number of Pages  Number of Data Points  \\\n",
      "0  04_NOVATEK_AR_2016_ENG_11              119                  24264   \n",
      "\n",
      "   Total Inference Time  Time per data point  Data points per sec  \n",
      "0            160.246681             0.006604           151.416553  \n",
      "                    PDF Name  Number of Pages  Number of Data Points  \\\n",
      "0  04_NOVATEK_AR_2016_ENG_11              119                  24264   \n",
      "\n",
      "   Total Inference Time  Time per data point  Data points per sec  \n",
      "0            160.224211             0.006603           151.437788  \n"
     ]
    }
   ],
   "source": [
    "for i in range(num_runs):\n",
    "    print(metrics_dfs[i].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0275679-d184-4cdb-8b3c-3e0e45bbff78",
   "metadata": {},
   "source": [
    "## 3. Running Benchmarks on KPI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2ed55-11a7-46fc-8338-5942992b1dfc",
   "metadata": {},
   "source": [
    "### 3.1. Setting up scratch directories and model for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "397ee606-3e80-4c6c-b484-3d862ad0ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_FOLDER = config.DATA_FOLDER / \"benchmark\"\n",
    "\n",
    "if not os.path.exists(BENCHMARK_FOLDER / \"infer_kpi\"):\n",
    "    pathlib.Path(BENCHMARK_FOLDER / \"infer_kpi\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb2f72-e636-4d0b-8c22-1c6df08a056e",
   "metadata": {},
   "source": [
    "### 3.2. KPI Infer Configurations and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7327374d-572c-4a11-b268-1718f7ec09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': '/opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/models/KPI_EXTRACTION'}\n",
      "/opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance\n",
      "/opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_kpi\n"
     ]
    }
   ],
   "source": [
    "file_config = QAFileConfig(\"infer_demo\")\n",
    "infer_config = QAInferConfig(\"infer_demo\")\n",
    "infer_config.relevance_dir = BENCHMARK_FOLDER / \"infer_relevance\"\n",
    "infer_config.result_dir = BENCHMARK_FOLDER / \"infer_kpi\"\n",
    "print(infer_config.load_dir)\n",
    "print(infer_config.relevance_dir)\n",
    "print(infer_config.result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6b6a7bf7-a2ab-45b1-9cc9-e4afbd346904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = pathlib.Path(file_config.saved_models_dir).parent\n",
    "model_rel_zip = pathlib.Path(model_root, 'KPI_EXTRACTION.zip')\n",
    "s3c.download_file_from_s3(model_rel_zip, config.CHECKPOINT_S3_PREFIX, \"KPI_EXTRACTION.zip\")\n",
    "with zipfile.ZipFile(pathlib.Path(model_root, 'KPI_EXTRACTION.zip'), 'r') as z:\n",
    "    z.extractall(model_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f77cf-1d01-4e16-890c-56b2576d3067",
   "metadata": {},
   "source": [
    "### 3.3. Defining Methods from the TextKPIInfer class for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "694c214f-4c12-43a0-876c-e46bf0e21645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_kpi_infer(n_best_per_sample=1):\n",
    "        \"\"\"Initialize TextKPIInfer class.\"\"\"\n",
    "\n",
    "        farm_logger = logging.getLogger(\"farm\")\n",
    "        farm_logger.setLevel(infer_config.farm_infer_logging_level)\n",
    "        global model\n",
    "        model = QAInferencer.load(\n",
    "            infer_config.load_dir[\"Text\"],\n",
    "            batch_size=infer_config.batch_size,\n",
    "            gpu=infer_config.gpu,\n",
    "            num_processes=infer_config.num_processes,\n",
    "        )\n",
    "        # num span-based candidate answer spans to consider from each passage\n",
    "        model.model.prediction_heads[0].n_best_per_sample = n_best_per_sample\n",
    "        # If positive, this will boost \"No Answer\" as prediction.\n",
    "        # If negative, this will decrease the model from giving \"No Answer\" as prediction.\n",
    "        model.model.prediction_heads[\n",
    "            0\n",
    "        ].no_ans_boost = infer_config.no_ans_boost\n",
    "        if not os.path.exists(infer_config.result_dir):\n",
    "            os.makedirs(infer_config.result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3865e5bd-18cf-4ecc-a320-f799f475c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_result(x):\n",
    "    \"\"\"Aggregate result method (helper function).\n",
    "\n",
    "    Helper function Used in `infer_on_relevance_results`\n",
    "    For relevant paragraphs related to a single pdf and question, find groups that the answer with highest score\n",
    "    is always no_answer. If that happens, we consider that question is not answerable for the given pdf.\n",
    "    \"\"\"\n",
    "    rank_1 = x[x[\"rank\"] == \"rank_1\"]\n",
    "    aggregated_no_answer = all(rank_1[\"answer\"] == \"no_answer\")\n",
    "    if aggregated_no_answer:\n",
    "        max_no_answer_score = rank_1[\"score\"].max()\n",
    "        return max_no_answer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f7d7b298-bf75-41cb-9d8a-faa104c6bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_relevance_results():\n",
    "        \"\"\"Make inference using the qa model on the relevant paragraphs.\n",
    "\n",
    "        Args:\n",
    "            relevance_results_dir (str): path to the directory where the csv file containing the relevant paragraphs\n",
    "            and KPIs for text are stored (output from the relevance stage).\n",
    "            kpi_df (Pandas.DataFrame): A dataframe with kpi questions\n",
    "        Returns:\n",
    "            span_df (Pandas.DataFrame): A dataframe, containing best n answers for each KPI question for each pdf.\n",
    "                The n is defined by top_k. The following columns are added:\n",
    "                    `answer_span`: answer span\n",
    "                    `score`: The score of span from qa model\n",
    "                    `rank`: for the given context and question, what is the rank of score for answer_span. For examples,\n",
    "                        if rank of a span is rank_1, it means that for the give context and question,\n",
    "                        the qa model gives the highest score to that span. while rank_2 means, the best guess of model\n",
    "                        is either`no_answer` or another span.\n",
    "\n",
    "        Note: The  result data frame will be saved in the `result_dir` directory.\n",
    "        \"\"\"\n",
    "        all_relevance_results_paths = glob.glob(\n",
    "            os.path.join(infer_config.relevance_dir, \"*.csv\")\n",
    "        )\n",
    "        all_span_dfs = []\n",
    "        num_csvs = len(all_relevance_results_paths)\n",
    "        metrics_df_list = []\n",
    "        _logger.info(\n",
    "            \"{} Starting KPI Inference for the following relevance CSV files found in {}:\\n{} \".format(\n",
    "                \"#\" * 20,\n",
    "                infer_config.relevance_dir,\n",
    "                [\n",
    "                    os.path.basename(relevance_results_path)\n",
    "                    for relevance_results_path in all_relevance_results_paths\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        for i, relevance_results_path in enumerate(all_relevance_results_paths):\n",
    "            _logger.info(\"{} {}/{}\".format(\"#\" * 20, i + 1, num_csvs))\n",
    "            pdf_name = os.path.basename(relevance_results_path).split(\n",
    "                \"_predictions_relevant\"\n",
    "            )[0]\n",
    "            predictions_file_name = \"{}_{}\".format(pdf_name, \"predictions_kpi.csv\")\n",
    "            if (\n",
    "                infer_config.skip_processed_files\n",
    "                and predictions_file_name in os.listdir(infer_config.result_dir)\n",
    "            ):\n",
    "                _logger.info(\n",
    "                    \"The KPI infer results for {} already exists. Skipping.\".format(\n",
    "                        pdf_name\n",
    "                    )\n",
    "                )\n",
    "                _logger.info(\n",
    "                    \"If you would like to re-process the already processed files, set \"\n",
    "                    \"`skip_processed_files` to False in the config file. \"\n",
    "                )\n",
    "                continue\n",
    "            _logger.info(\"Starting KPI Extraction for {}\".format(pdf_name))\n",
    "            \n",
    "            \n",
    "            input_df = pd.read_csv(relevance_results_path)\n",
    "            column_names = [\"text_b\", \"text\", \"page\", \"pdf_name\", \"source\"]\n",
    "\n",
    "            if len(input_df) == 0:\n",
    "                _logger.info(\n",
    "                    \"The received relevance file is empty for {}\".format(pdf_name)\n",
    "                )\n",
    "                df_empty = pd.DataFrame([])\n",
    "                df_empty.to_csv(os.path.join(infer_config.result_dir, predictions_file_name))\n",
    "                continue\n",
    "\n",
    "            assert set(column_names).issubset(\n",
    "                set(input_df.columns)\n",
    "            ), \"\"\"The result of relevance detector has {} columns,\n",
    "            while expected {}\"\"\".format(\n",
    "                input_df.columns, column_names\n",
    "            )\n",
    "\n",
    "            qa_dict = [\n",
    "                {\"qas\": [question], \"context\": context}\n",
    "                for question, context in zip(input_df[\"text\"], input_df[\"text_b\"])\n",
    "            ]\n",
    "            num_data_points = len(qa_dict)\n",
    "            result = []\n",
    "            chunk_size = 1000\n",
    "            chunk_idx = 0\n",
    "            total_file_time = 0\n",
    "            while chunk_idx * chunk_size < num_data_points:\n",
    "                chunk_start = time.time()\n",
    "                \n",
    "                data_chunk = qa_dict[\n",
    "                    chunk_idx * chunk_size : (chunk_idx + 1) * chunk_size\n",
    "                ]\n",
    "                predictions_chunk = model.inference_from_dicts(dicts=data_chunk)\n",
    "                result.extend(predictions_chunk)\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                chunk_end = time.time()\n",
    "                total_file_time += (chunk_end - chunk_start)\n",
    "            # result = self.model.inference_from_dicts(dicts=qa_dict)\n",
    "\n",
    "            time_per_data_point = total_file_time / num_data_points\n",
    "            data_points_per_sec = 1/time_per_data_point\n",
    "            \n",
    "            _logger.info(\"Ran inference on the file {} with {} relevant data points in {} sec. ({} sec per data point, {} data points per sec)\".format(\n",
    "                pdf_name, num_data_points, total_file_time, time_per_data_point, data_points_per_sec\n",
    "            ))\n",
    "            \n",
    "            metrics_list = [[pdf_name, num_data_points, total_file_time, time_per_data_point, data_points_per_sec]]\n",
    "            metrics_df = pd.DataFrame(metrics_list, columns = ['PDF Name', 'Number of Data Points', 'Total Inference Time', 'Time per data point', 'Data points per sec'])\n",
    "            metrics_df_list.append(metrics_df)\n",
    "            \n",
    "            head_num = 0\n",
    "            num_answers = model.model.prediction_heads[0].n_best_per_sample + 1\n",
    "            answers_dict = defaultdict(list)\n",
    "\n",
    "            for exp in result:\n",
    "                preds = exp[\"predictions\"][head_num][\"answers\"]\n",
    "                # Get the no_answer_score\n",
    "                no_answer_score = [\n",
    "                    p[\"score\"] for p in preds if p[\"answer\"] == \"no_answer\"\n",
    "                ]\n",
    "                if (\n",
    "                    len(no_answer_score) == 0\n",
    "                ):  # Happens if no answer is not among the n_best predictions.\n",
    "                    no_answer_score = (\n",
    "                        preds[0][\"score\"] - exp[\"predictions\"][head_num][\"no_ans_gap\"]\n",
    "                    )\n",
    "                else:\n",
    "                    no_answer_score = no_answer_score[0]\n",
    "\n",
    "                # Based on Farm implementation, no_answer_score already is equal = \"CLS score\" + no_ans_boost\n",
    "                # https://github.com/deepset-ai/FARM/blob/978da5d7600c48be458688996538770e9334e71b/farm/modeling/prediction_head.py#L1348\n",
    "                pure_no_ans_score = no_answer_score - infer_config.no_ans_boost\n",
    "\n",
    "                for i in range(\n",
    "                    num_answers\n",
    "                ):  # This param is not exactly representative, n_best mostly defines num answers.\n",
    "                    answers_dict[f\"rank_{i+1}\"].append(\n",
    "                        (\n",
    "                            preds[i][\"answer\"],\n",
    "                            preds[i][\"score\"],\n",
    "                            pure_no_ans_score,\n",
    "                            no_answer_score,\n",
    "                        )\n",
    "                    )\n",
    "            for i in range(num_answers):\n",
    "                input_df[f\"rank_{i+1}\"] = answers_dict[f\"rank_{i+1}\"]\n",
    "\n",
    "            # Let's put different kpi predictions and their scores into one column so we can sort them.\n",
    "            var_cols = [i for i in list(input_df.columns) if i.startswith(\"rank_\")]\n",
    "            id_vars = [i for i in list(input_df.columns) if not i.startswith(\"rank_\")]\n",
    "            input_df = pd.melt(\n",
    "                input_df,\n",
    "                id_vars=id_vars,\n",
    "                value_vars=var_cols,\n",
    "                var_name=\"rank\",\n",
    "                value_name=\"answer_score\",\n",
    "            )\n",
    "\n",
    "            # Separate a column with tuple value into two columns\n",
    "            input_df[\n",
    "                [\"answer\", \"score\", \"no_ans_score\", \"no_answer_score_plus_boost\"]\n",
    "            ] = pd.DataFrame(input_df[\"answer_score\"].tolist(), index=input_df.index)\n",
    "            input_df = input_df.drop(columns=[\"answer_score\"], axis=1)\n",
    "\n",
    "            no_answerables = (\n",
    "                input_df.groupby([\"pdf_name\", \"text\"])\n",
    "                .apply(lambda grp: aggregate_result(grp))\n",
    "                .dropna(how=\"all\")\n",
    "            )\n",
    "            no_answerables = pd.DataFrame(\n",
    "                no_answerables, columns=[\"score\"]\n",
    "            ).reset_index()\n",
    "            no_answerables[\"answer\"] = \"no_answer\"\n",
    "            no_answerables[\"source\"] = \"Text\"\n",
    "\n",
    "            # Filter to span-based answers\n",
    "            span_df = input_df[input_df[\"answer\"] != \"no_answer\"]\n",
    "            # Concatenate the result of span answers with non answerable examples.\n",
    "            span_df = pd.concat([span_df, no_answerables], ignore_index=True)\n",
    "\n",
    "            # Get the predictions with n highest score for each pdf and question.\n",
    "            # If the question is considered unanswerable, the best prediction is \"no_answer\", but the best span-based answer\n",
    "            # is also returned. if the question is answerable, the best span-based answers are returned.\n",
    "            span_df = (\n",
    "                span_df.groupby([\"pdf_name\", \"text\"])\n",
    "                .apply(lambda grp: grp.nlargest(infer_config.top_k, \"score\"))\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Final cleaning on the dataframe, removing unnecessary columns and renaming `text` and `text_b` columns.\n",
    "            unnecessary_cols = [\"rank\"] + [\n",
    "                i for i in list(span_df.columns) if i.startswith(\"Unnamed\")\n",
    "            ]\n",
    "            span_df = span_df.drop(columns=unnecessary_cols, axis=1)\n",
    "            span_df.rename(columns={\"text\": \"kpi\", \"text_b\": \"paragraph\"}, inplace=True)\n",
    "\n",
    "            # Add the kpi id\n",
    "            reversed_kpi_mapping = {\n",
    "                value[0]: key\n",
    "                for key, value in get_kpi_mapping_category(kpi_df)[\n",
    "                    \"KPI_MAPPING\"\n",
    "                ].items()\n",
    "            }\n",
    "            span_df[\"kpi_id\"] = span_df[\"kpi\"].map(reversed_kpi_mapping)\n",
    "\n",
    "            # Change the order of columns\n",
    "            first_cols = [\"pdf_name\", \"kpi\", \"kpi_id\", \"answer\", \"page\"]\n",
    "            column_order = first_cols + [\n",
    "                col for col in span_df.columns if col not in first_cols\n",
    "            ]\n",
    "            span_df = span_df[column_order]\n",
    "\n",
    "            result_path = os.path.join(infer_config.result_dir, predictions_file_name)\n",
    "            span_df.to_csv(result_path)\n",
    "            _logger.info(\"Save the result of KPI extraction to {}\".format(result_path))\n",
    "            all_span_dfs.append(span_df)\n",
    "        concatenated_dfs = (\n",
    "            pd.concat(all_span_dfs) if len(all_span_dfs) > 0 else pd.DataFrame()\n",
    "        )\n",
    "        metrics_df = pd.DataFrame()\n",
    "        if len(metrics_df_list) > 0:\n",
    "            metrics_df = pd.concat(metrics_df_list) if len(metrics_df_list) > 0 else pd.DataFrame()\n",
    "            _logger.info(\n",
    "                \"Metrics for KPI from revelant paragraphs are: \\nTotal Number of Data Points Processed = {} \\nTotal Inference Time = {} \\nAverage Inference Time Per CSV = {} \\nMinimum Inference Time of CSV = {} \\nMaximum Inference Time of CSV = {} \\nStd of Inference Times of CSVs = {} \\nAverage Time Per Data Point Processed= {} \\nAverage Data Points Processed Per Second = {} \\n\"\n",
    "                .format(metrics_df['Number of Data Points'].sum(), \n",
    "                        metrics_df['Total Inference Time'].sum(),\n",
    "                        metrics_df['Total Inference Time'].mean(),\n",
    "                        metrics_df['Total Inference Time'].min(),\n",
    "                        metrics_df['Total Inference Time'].max(),\n",
    "                        metrics_df['Total Inference Time'].std(),\n",
    "                        metrics_df['Time per data point'].mean(),\n",
    "                        metrics_df['Data points per sec'].mean()\n",
    "                        )\n",
    "            )\n",
    "        model.close_multiprocessing_pool()\n",
    "        return concatenated_dfs, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dfc298b5-e6fb-48eb-bbf8-b601fdbda672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_for_kpi():\n",
    "    if not os.path.exists(BENCHMARK_FOLDER / \"infer_kpi\"):\n",
    "        pathlib.Path(BENCHMARK_FOLDER / \"infer_kpi\").mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(BENCHMARK_FOLDER / \"infer_kpi\"):\n",
    "        file_path = os.path.join(BENCHMARK_FOLDER / \"infer_kpi\", filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33149241-ca0b-4331-8b6f-17514cbc9a4c",
   "metadata": {},
   "source": [
    "### 3.4. Running Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f1452526-c75e-42be-999c-7a9339d84bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/08/2022 09:25:46 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "08/08/2022 09:25:47 - WARNING - farm.infer -   QAInferencer always has task_type='question_answering' even if another value is provided to Inferencer.load() or QAInferencer()\n",
      "08/08/2022 09:25:47 - INFO - __main__ -   #################### Starting KPI Inference for the following relevance CSV files found in /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance:\n",
      "['04_NOVATEK_AR_2016_ENG_11_predictions_relevant.csv'] \n",
      "08/08/2022 09:25:47 - INFO - __main__ -   #################### 1/1\n",
      "08/08/2022 09:25:47 - INFO - __main__ -   Starting KPI Extraction for 04_NOVATEK_AR_2016_ENG_11\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  4.96 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  8.47 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  6.73 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  7.26 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  6.48 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  7.03 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  8.23 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  6.66 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  6.71 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  8.01 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.90 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.75 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.67 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.88 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.73 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.74 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.84 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.81 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.73 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.66 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.86 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.89 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.77 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.79 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.80 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.70 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.75 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.80 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.78 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.81 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.72 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.84 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.74 Batches/s]\n",
      "Inferencing Samples: 100%|| 2/2 [00:00<00:00, 15.74 Batches/s]\n",
      "08/08/2022 09:25:57 - INFO - __main__ -   Ran inference on the file 04_NOVATEK_AR_2016_ENG_11 with 404 relevant data points in 9.389342308044434 sec. (0.023240946307040676 sec per data point, 43.027507864301434 data points per sec)\n",
      "08/08/2022 09:25:57 - INFO - __main__ -   Save the result of KPI extraction to /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_kpi/04_NOVATEK_AR_2016_ENG_11_predictions_kpi.csv\n",
      "08/08/2022 09:25:57 - INFO - __main__ -   Metrics for KPI from revelant paragraphs are: \n",
      "Total Number of Data Points Processed = 404 \n",
      "Total Inference Time = 9.389342308044434 \n",
      "Average Inference Time Per CSV = 9.389342308044434 \n",
      "Minimum Inference Time of CSV = 9.389342308044434 \n",
      "Maximum Inference Time of CSV = 9.389342308044434 \n",
      "Std of Inference Times of CSVs = nan \n",
      "Average Time Per Data Point Processed= 0.023240946307040676 \n",
      "Average Data Points Processed Per Second = 43.027507864301434 \n",
      "\n",
      "08/08/2022 09:26:05 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "08/08/2022 09:26:06 - WARNING - farm.infer -   QAInferencer always has task_type='question_answering' even if another value is provided to Inferencer.load() or QAInferencer()\n",
      "08/08/2022 09:26:06 - INFO - __main__ -   #################### Starting KPI Inference for the following relevance CSV files found in /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_relevance:\n",
      "['04_NOVATEK_AR_2016_ENG_11_predictions_relevant.csv'] \n",
      "08/08/2022 09:26:06 - INFO - __main__ -   #################### 1/1\n",
      "08/08/2022 09:26:06 - INFO - __main__ -   Starting KPI Extraction for 04_NOVATEK_AR_2016_ENG_11\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  5.64 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  7.14 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  5.97 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  8.19 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  7.07 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  6.51 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  6.76 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  7.35 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00,  7.80 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 10.21 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.89 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.74 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.73 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.77 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.76 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.70 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.80 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.78 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.80 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.75 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.75 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.78 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.62 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.81 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.88 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.72 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.72 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.76 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.64 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.74 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.65 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.90 Batches/s]\n",
      "Inferencing Samples: 100%|| 3/3 [00:00<00:00, 15.82 Batches/s]\n",
      "Inferencing Samples: 100%|| 2/2 [00:00<00:00, 15.86 Batches/s]\n",
      "08/08/2022 09:26:16 - INFO - __main__ -   Ran inference on the file 04_NOVATEK_AR_2016_ENG_11 with 404 relevant data points in 9.409790515899658 sec. (0.023291560682919946 sec per data point, 42.934005737679705 data points per sec)\n",
      "08/08/2022 09:26:16 - INFO - __main__ -   Save the result of KPI extraction to /opt/app-root/src/aicoe-osc-demo-2022-07-13-21-52/data/benchmark/infer_kpi/04_NOVATEK_AR_2016_ENG_11_predictions_kpi.csv\n",
      "08/08/2022 09:26:16 - INFO - __main__ -   Metrics for KPI from revelant paragraphs are: \n",
      "Total Number of Data Points Processed = 404 \n",
      "Total Inference Time = 9.409790515899658 \n",
      "Average Inference Time Per CSV = 9.409790515899658 \n",
      "Minimum Inference Time of CSV = 9.409790515899658 \n",
      "Maximum Inference Time of CSV = 9.409790515899658 \n",
      "Std of Inference Times of CSVs = nan \n",
      "Average Time Per Data Point Processed= 0.023291560682919946 \n",
      "Average Data Points Processed Per Second = 42.934005737679705 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 2\n",
    "kpi_metrics_dfs = []\n",
    "for i in range(num_runs):\n",
    "    cleanup_for_kpi()\n",
    "    initialize_kpi_infer()\n",
    "    result_df, kpi_metrics_df = infer_on_relevance_results()\n",
    "    kpi_metrics_dfs.append(kpi_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e60b46a-3bef-428d-81c6-3f26c04f1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    PDF Name  Number of Data Points  Total Inference Time  \\\n",
      "0  04_NOVATEK_AR_2016_ENG_11                    404              9.389342   \n",
      "\n",
      "   Time per data point  Data points per sec  \n",
      "0             0.023241            43.027508  \n",
      "                    PDF Name  Number of Data Points  Total Inference Time  \\\n",
      "0  04_NOVATEK_AR_2016_ENG_11                    404              9.409791   \n",
      "\n",
      "   Time per data point  Data points per sec  \n",
      "0             0.023292            42.934006  \n"
     ]
    }
   ],
   "source": [
    "for i in range(num_runs):\n",
    "    print(kpi_metrics_dfs[i].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
